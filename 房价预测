###File Description###

import numpy as np
import pandas as pd

#read file

train_df = pd.read_csv('filename', index_col=0)
test_df = pd.read_csv('filename', index_col=0)

#through the data
tran_df.head()  #first 5 rows infomation

#merge data  看完数据，可以考虑如何处理数据。先合并test data 和 train data，方便做数据清洗和处理。
%matplotlib inline
prices = pd.Dataframe({"price":train_df["SalePrice"],"log(price+1)":np.loglp(train_df["SalePrice"])})
prices.hist()
#log用于预处理y值，+1是为了预防y等于0的情况，进行normalize变成偏正态化，平滑数据。最后的结果需要把这些log后的数据变回去，log1p()就需要expm1(),log()就需要exp()。

y_train = np.log1p(train_df.pop(‘SalePrice'))  #pandas.pop 剪切saleprice这一column，复制到y_train中去。
all_df = pd.concat((train_df, test_df), axis=0)  
all_df.shape

y_train.head()
all_df["MSSubclass"].dtype
#改变dtype从int为str
all_df["MSSubclass"] = all_df["MSSubclass"].astype(str)
all_df['MSSubClass'].value_counts()

pd.get_dummies(all_df['MSSubclass'], prefix='MSSubclass').head()
all_dummy_df = pd.get_dummies(all_df)
all_dummy_df.head()

#检查numerical数据有没有问题
all_dummy_df.isnull().sum().sort_values(ascending=False).head(10)

mean_cols = all_dummy_df.mean()
mean_cols.head()

all_dummy_df = all_dummy_df.fillna(mean_cols) #fill the nan or lost.

#normalization of numerical data

numeric_cols = all_df.columns[all_df.dtypes != 'object' ]
numeric_col_means = all_dummy_df.loc[:,numeric_cols].mean()
numeric_col_std = all_dummy_df.loc[:,numeric_cols].std()
all_dummy_df.loc[:, numeiric_cols] = (all_dummy_df[:,numeric_cols] - numeric_cols_means) / numeric_col_std

#separate the train_data and text_data
dummy_train_df = all_dummy_df.loc[train_df.index]
dummy_test_df = all_dummt_df.loc[test_df.index]
dummy_train_df.shape
dummy_test_df.shape


#Ridge Regression to test data
from sklearn.linear.model import Ridge
from sklearn.model_selection import cross_val_score

X_train = dummy_train_df.values  #numpy data
X_test = dummy_test_df.values

#cross validation and grid search
alphas = np.logspace(-3,2,50)
test_scores = []
for alpha in alphas:
    clf = Ridge(alpha)
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))  #mse
    test_scores.append(np.mean(test_score))
 

#adjust parameters

import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(alphas, test_scores)
plt.title('Alpha vs CV Error')

#Random Forest to test data

from sklearn.ensemble import RandomForestRegressor
max_features = [.1, .3, .5, .7, .9, .99]
test_scores = []
for max_feat in max_features:
    clf = RandomForestRegressor(n_estimators=200, max_features=max_feat)
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))

#Ensemble by stacking

ridge = Ridge(alpha=15)
rf = RandomForestRegressor(n_estimators=500, max_features=.3)
ridge.fit(X_train, y_train)
rf.fit(X_train, y_train)
y_ridge = np.expm1(ridge.predict(X_test))
y_rf = np.expm1(rf.predict(X_test))
y_final = (y_ridge + y_rf) / 2

#提交结果
submission_df = pd.DataFrame(data= {'Id' : test_df.index, 'SalePrice': y_final})

---------------------------------------------------------------------------------------------------------
###其他kaggle竞赛的做法

from sklearn.linear_model import Ridge
ridge = Ridge(15)

#diference between regressor and classifier 
from sklearn.ensemble import BaggingRegressor
from sklearn.model_selection import cross_val_score

params = [1, 10, 15, 20, 25, 30, 40]  #numbers of base estimater
test_scores = []
for param in params:
    clf = BaggingRegressor(n_estimators=param, base_estimator=ridge)
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))

#plot to decide whether use bagging

import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(params, test_scores)
plt.title("n_estimator vs CV Error");

#boosting

from sklearn.ensemble import AdaBoostRegressor

params = [10, 15, 20, 25, 30, 35, 40, 45, 50]
test_scores = []
for param in params:
    clf = BaggingRegressor(n_estimators=param, base_estimator=ridge)
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))

plt.plot(params, test_scores)
plt.title("n_estimator vs CV Error");

#屌屌的XGBoost

from xgboost import XGBRegressor

params = [1,2,3,4,5,6]
test_scores = []
for param in params:
    clf = XGBRegressor(max_depth=param)
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))
    
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(params, test_scores)
plt.title("max_depth vs CV Error");



    
 


